{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Presentation du projet d'indexion de texte</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from ipywidgets import FloatProgress\n",
    "from IPython.display import display\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "import nltk\n",
    "import string\n",
    "import operator\n",
    "import shutil,os\n",
    "import re\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Represent the constraint of memory\n",
    "NB_DOCUMENT = 1000\n",
    "DATA_PATH = \"../data/latimes/\"\n",
    "WRITING_PATH_POSTING_LIST = \"../data/save/\"\n",
    "NAME_POSTING_LIST = \"postingList_\"\n",
    "SEPARATOR = \" \"\n",
    "NAME_DOC_LIST = \"docList\"\n",
    "\n",
    "# link the tags with the importance in the text.\n",
    "TAGS_IMPORTANCE = {  'headline': 3,\n",
    "                     'text': 1,\n",
    "                     'section':1,\n",
    "                     'graphic':2\n",
    "                  }\n",
    "STOP_WORDS = stopwords.words('english') + list(string.punctuation)\n",
    "STEMMER = PorterStemmer()\n",
    "TAG_NUMBER = \"NUMBER\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "# clean repository of the giver path: \"folder\"\n",
    "###\n",
    "def cleanRepository(folder):\n",
    "    for the_file in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, the_file)\n",
    "        try:\n",
    "            if os.path.isfile(file_path):\n",
    "                os.unlink(file_path)\n",
    "            #elif os.path.isdir(file_path): shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "# Tokenize a sentense.\n",
    "##\n",
    "def tokenizeWord(paragraphContent):  \n",
    "    # We tokenize and remove the stop word\n",
    "    words = [word for word in word_tokenize(paragraphContent.lower()) if word not in STOP_WORDS]\n",
    "    \n",
    "    # nlkt does not decompose the hyphen.\n",
    "    splitHiphen = []\n",
    "    for word in words:\n",
    "        if '-' in word:\n",
    "            for decomposedWord in word.split('-'):\n",
    "                splitHiphen.append(decomposedWord)\n",
    "        else:\n",
    "            splitHiphen.append(word)  \n",
    "            \n",
    "    return splitHiphen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "# Format the text in the right form.\n",
    "# Tokenize and stem the text\n",
    "# Update the voc list passed in parameter.\n",
    "##\n",
    "def handleFormatText(paragraphContent, vocList, docLenght, docId):  \n",
    "    # We tokenize and remove the stop word\n",
    "    words = tokenizeWord(paragraphContent) \n",
    "    \n",
    "    stemWords = []\n",
    "    # We loop on each word.\n",
    "    for word in words:\n",
    "        stemWord = STEMMER.stem(word)\n",
    "        \n",
    "        # Selection on a part of string.\n",
    "        stemWord = re.sub(\"[*\\'\\.+:,\\`:/]\", '', stemWord)\n",
    "        if stemWord.isdigit() or len(stemWord) < 2:\n",
    "            continue\n",
    "            \n",
    "        stemWords.append(stemWord)\n",
    "        # Update the listVoc\n",
    "        if stemWord in vocList:\n",
    "            vocList[stemWord] = vocList[stemWord] + 1\n",
    "        else:\n",
    "            vocList[stemWord] = 1\n",
    "        \n",
    "        docLenght[docId] += 1\n",
    "    return stemWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "# The function add the entry in the correct posting list\n",
    "##\n",
    "def buildPostingList(stemWords, currentDict, idDoc):\n",
    "    # We update the stemWords.\n",
    "    for word in stemWords:\n",
    "        # The word have already been seen, we update thedict\n",
    "        if word in currentDict :\n",
    "            # We update the dict reprensenting the posting list.\n",
    "            if idDoc in currentDict[word]:\n",
    "                currentDict[word][idDoc] = currentDict[word][idDoc] + 1\n",
    "\n",
    "            else:\n",
    "                currentDict[word][idDoc] = 1\n",
    "\n",
    "        # We don't have word for now\n",
    "        else:\n",
    "            currentDict[word] = {idDoc : 1};\n",
    "            \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "# Write file.\n",
    "##\n",
    "def writingInFile(currentDict, index, path, name, separator):  \n",
    "    # sort word for the posting list.\n",
    "    sorted_word = sorted(currentDict.keys())\n",
    "    \n",
    "    # write the posting list.\n",
    "    with open(path+name+str(index),\"a+\") as f:\n",
    "        for word in sorted_word:\n",
    "            portingEntry = word + separator\n",
    "            for docID, value in currentDict[word].items():\n",
    "                portingEntry = portingEntry + str(docID) + separator + str(value) + separator\n",
    "            f.write(portingEntry + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "# Write voc file\n",
    "##\n",
    "def writingDictInFile(currentDict, path, name, separator): \n",
    "    # write the posting list.\n",
    "    with open(path+name ,\"a+\") as f:\n",
    "        for docID, value in currentDict.items():\n",
    "            portingEntry = \"\"\n",
    "            portingEntry = portingEntry + str(docID) + separator + str(value)\n",
    "            f.write(portingEntry + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "# The function build the index file composed by the voc and the associated posting list.\n",
    "##\n",
    "def buildIndexFile(vocList, docLenght) :\n",
    "    print(\"Building index File\")\n",
    "    \n",
    "    # We get the list of file containing the articles.\n",
    "    articles = [DATA_PATH + file for file in listdir(DATA_PATH) if (isfile(join(DATA_PATH, file)) and \".txt\" not in file and \".DS_Store\" not in file )]\n",
    "    progress_bar = FloatProgress(min=0, max=len(articles))\n",
    "    display(progress_bar)\n",
    "    \n",
    "    # List containing the term and the number of time it appear.\n",
    "    currentPostingList = {}\n",
    "    counter = 0\n",
    "    docIDCounter = 0 \n",
    "    \n",
    "    \n",
    "    #We loop on each document composing the corpus.\n",
    "    for article in articles:\n",
    "        with open(article) as curArticle:\n",
    "            file = curArticle.read()\n",
    "            fileXML = bs(file,\"lxml\")\n",
    "            \n",
    "            # We loop on each doc tag\n",
    "            for document in fileXML.findAll('doc'):\n",
    "                docIDCounter = docIDCounter + 1\n",
    "                docID = document.find(\"docid\").string\n",
    "                docLenght[docID] = 0\n",
    "                \n",
    "                # get the text containing in the current article\n",
    "                curParagraph = document.find_all('p')\n",
    "                for paragraph in curParagraph:\n",
    "                   \n",
    "                    # We balance with the importance of the parent tag\n",
    "                    if paragraph.parent.name in TAGS_IMPORTANCE:\n",
    "                        for index in range(TAGS_IMPORTANCE[paragraph.parent.name]):\n",
    "                            stemWords = handleFormatText(paragraph.string,vocList, docLenght, docID)\n",
    "                            buildPostingList(stemWords, currentPostingList, int(docID))\n",
    "                             \n",
    "                if docIDCounter % NB_DOCUMENT == 0 :\n",
    "                    counter = counter + 1\n",
    "                    writingInFile(currentPostingList, counter, WRITING_PATH_POSTING_LIST, NAME_POSTING_LIST, SEPARATOR)\n",
    "                    # clear the ram memory.\n",
    "                    currentPostingList.clear()\n",
    "             \n",
    "        curArticle.closed\n",
    "        progress_bar.value += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eprint(*args, **kwargs):\n",
    "    print(*args, file=sys.stderr, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PostingList(object):\n",
    "    # Args :\n",
    "    # qt is a string containing the query term this PL is made for\n",
    "    # ordered_list is a list of (score, doc_id) ordered in decreasing score\n",
    "    # access_dict (optional, can be computed from ordered_list) is a dict associating a doc to its score in this PL\n",
    "    def __init__(self, qt, ordered_list, access_dict=None):\n",
    "        self.qt=qt\n",
    "        self.ordered_list = ordered_list\n",
    "        if access_dict is not None:\n",
    "            self.access_dict = access_dict\n",
    "        else:\n",
    "            self.access_dict = {}\n",
    "            for score,doc in ordered_list:\n",
    "                assert doc not in self.access_dict\n",
    "                self.access_dict[doc] = score\n",
    "\n",
    "        self.docs_visited = set()\n",
    "        self.ordered_idx = 0\n",
    "\n",
    "    # Returns : A (score, doc_index) tuple corresponding to the first non-visited entry in the ordered traversal\n",
    "    def seek_next(self):\n",
    "        while self.ordered_list[self.ordered_idx][1] in self.docs_visited:\n",
    "            self.ordered_idx += 1\n",
    "        return self.ordered_list[self.ordered_idx]\n",
    "\n",
    "    # Returns : The score of the item preceding the next ordered accessed item\n",
    "    def next_item_predecessor_score(self):\n",
    "        tmp_idx = self.ordered_idx\n",
    "        while self.ordered_list[tmp_idx][1] in self.docs_visited:\n",
    "            self.ordered_idx += 1\n",
    "        return self.ordered_list[tmp_idx-1][0]\n",
    "\n",
    "    # Args :\n",
    "    # doc_id is an integer containing the id of the document we want to mark as visited in the sorted access\n",
    "    def mark_visited(self, doc_id):\n",
    "        assert doc_id not in self.docs_visited\n",
    "        self.docs_visited.add(doc_id)\n",
    "\n",
    "    # Args :\n",
    "    # doc_id is an integer containing the document id to lookup in the random access\n",
    "    #\n",
    "    # Returns : The score of the queried document in the PL\n",
    "    def random_lookup(self, doc_id):\n",
    "        return self.access_dict[doc_id]\n",
    "\n",
    "#TODO(mathishammel): Use a real priority queue. Lists are disgusting\n",
    "class TopEntries(object):\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        self.top = []\n",
    "\n",
    "    def insert(self, priority, element):\n",
    "        self.top += [(priority, element)]\n",
    "        self.top = sorted(self.top, reverse=True)[:self.k]\n",
    "\n",
    "    def pop_lowest(self):\n",
    "        res = self.top[-1]\n",
    "        del self.top[-1]\n",
    "        return res\n",
    "\n",
    "    def get_min_score(self):\n",
    "        if len(self.top) == 0:\n",
    "            return -1.0\n",
    "        return self.top[-1][0]\n",
    "\n",
    "def calc_avg(lst):\n",
    "    return float(sum(lst))/len(lst)\n",
    "\n",
    "# Args :\n",
    "# posting_lists is a list of PostingList objects corresponding to the PLs for all query terms.\n",
    "# k is an integer containing the length of the desired top-k ranking\n",
    "#\n",
    "# Returns : A list containing the (total_score, doc_id) for the top-k elements\n",
    "def top_k_thresh(posting_lists, k, epsilon=0.0):\n",
    "    #Check if we're not trying to get an impossible top-k\n",
    "    for posting_list in posting_lists:\n",
    "        assert k < len(posting_list.ordered_list)\n",
    "\n",
    "    top_k = TopEntries(k)\n",
    "    top_non_visited = []\n",
    "    for posting_list in posting_lists:\n",
    "        top_non_visited.append(posting_list.seek_next())\n",
    "\n",
    "    eprint('Initialized top inorder array :', top_non_visited)\n",
    "    threshold = 1e999 # Close enough to infinity, hopefully...\n",
    "    \n",
    "    while top_k.get_min_score() < threshold / (1.0 + epsilon):\n",
    "        eprint('Starting new round')\n",
    "        selected_idx = -1\n",
    "        best_indiv_in_order = -1.0 # Assuming all PL scores are positive\n",
    "        for idx, score in enumerate(top_non_visited):\n",
    "            if best_indiv_in_order < score:\n",
    "                selected_idx = idx\n",
    "                best_indiv_in_order = score\n",
    "        selected_element = top_non_visited[selected_idx]\n",
    "        selected_score, selected_doc_id = selected_element\n",
    "        eprint('  Selected PL index is', selected_idx)\n",
    "        eprint('  Selected element is', selected_element)\n",
    "        posting_lists[selected_idx].mark_visited(top_non_visited[selected_idx][1])\n",
    "        top_non_visited[selected_idx] = posting_lists[selected_idx].seek_next()\n",
    "\n",
    "        scores = []\n",
    "        for idx in range(len(posting_lists)):\n",
    "            if idx == selected_idx:\n",
    "                scores.append(selected_score)\n",
    "                continue\n",
    "            scores.append(posting_lists[idx].random_lookup(selected_doc_id))\n",
    "            posting_lists[idx].mark_visited(selected_doc_id)\n",
    "            if top_non_visited[idx][1] == selected_doc_id:\n",
    "                top_non_visited[idx] = posting_lists[idx].seek_next()\n",
    "        \n",
    "        eprint('  Individual scores for document',selected_doc_id,'are', scores)\n",
    "        tot_score = calc_avg(scores)\n",
    "        eprint('  Average score is', tot_score)\n",
    "        top_k.insert(tot_score, selected_doc_id)\n",
    "        eprint('  Current top K is', top_k.top)\n",
    "\n",
    "        all_lists_ready = True\n",
    "        next_prev_scores = []\n",
    "        for idx,posting_list in enumerate(posting_lists):\n",
    "            if posting_list.ordered_list[0][1] not in posting_list.docs_visited:\n",
    "                eprint('  Posting list',idx,'is not ready for threshold computation yet, aborting.')\n",
    "                all_lists_ready = False\n",
    "                break\n",
    "            next_prev_scores.append(posting_list.next_item_predecessor_score())\n",
    "        eprint('  Nextprev scores are',next_prev_scores)\n",
    "        threshold = calc_avg(next_prev_scores)\n",
    "    return top_k.top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Test Fagin's 2 on ppt example\n",
    "pl1 = PostingList('hello', [(0.9,2),(0.8,5),(0.7,6),(0.6,4),(0.5,1),(0.4,3)])\n",
    "pl2 = PostingList('world', [(0.85,3),(0.8,5),(0.75,2),(0.74,6),(0.74,1),(0.7,4)])\n",
    "\n",
    "print top_k_thresh([pl1, pl2], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Creation du index File</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleanRepository(WRITING_PATH_POSTING_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocList = {}\n",
    "docLenght = {}\n",
    "buildIndexFile(vocList, docLenght)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "writingDictInFile(docLenght, WRITING_PATH_POSTING_LIST, NAME_DOC_LIST, \" \")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
